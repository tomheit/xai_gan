{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b83978d-4ad0-4805-9998-553cd7775568",
   "metadata": {},
   "source": [
    "# CNN and GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b72ce68-17fc-4529-89a5-cc69bb9d6b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Conv2DTranspose, LeakyReLU, Reshape\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fde18e8f-c18f-4f46-b243-197debe7843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the mnist dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train_classes), (x_test, y_test_classes) = mnist.load_data()\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# preparation\n",
    "num_classes = 10\n",
    "num_train = x_train.shape[0]\n",
    "num_test = x_train.shape[0]\n",
    "img_width = x_train.shape[1]\n",
    "img_height = x_train.shape[2]\n",
    "img_size_flat = img_width * img_height\n",
    "\n",
    "# one-hot encoding\n",
    "y_train = np.eye(num_classes, dtype = float)[y_train_classes]\n",
    "y_test = np.eye(num_classes, dtype = float)[y_test_classes]\n",
    "\n",
    "# normalize input to [0,1]\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# create tf dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcedf1f-7c80-40e1-8de2-9768ad610627",
   "metadata": {},
   "source": [
    "preparing data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dadc9b63-7c08-457b-b654-267f46ca3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(img):\n",
    "    #VARIABILITY = 50\n",
    "    #deviation = VARIABILITY * random.random()\n",
    "    noise = np.random.normal(0, 0.1, img.shape)\n",
    "    result = img + noise\n",
    "    np.clip(result, 0, 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f79581b-2dc7-4cd5-b532-f190b4f146a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation: shifting, rotating and zooming into the images\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomRotation(0.125, fill_mode = 'constant'),\n",
    "    layers.experimental.preprocessing.RandomTranslation(0.2, 0.2, fill_mode = 'constant'),\n",
    "    layers.experimental.preprocessing.RandomZoom((0.2, -0.1), (0.2, -0.1), fill_mode = 'constant')\n",
    "])\n",
    "\n",
    "def prepare_data(ds):\n",
    "    ds = ds.map(lambda x, y: (data_augmentation(x, training = True), y), num_parallel_calls = tf.data.AUTOTUNE)\n",
    "    return ds.prefetch(buffer_size = tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24fe02f7-4c70-4312-a4b0-7743116820e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.9960777\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAACkCAYAAABPav1bAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAATbklEQVR4nO3dcYyV1ZnH8d9TRA1DrUAVKCpUS3CVutRSpYq6Rt26pI1iu6ZsaqUlpUkloalpatw0K3/Ykq26CdE00KirTde6iRoIJotGadlNDXFKKRVmLVirolNHQkGGUnDg2T/mws57zztz79z73nPuy/1+EjJznjkz7wF+ec+89z33vObuAgAAcX0o9QAAAOhETMAAACTABAwAQAJMwAAAJMAEDABAAkzAAAAk0NQEbGY3mtmrZrbLzO4qalDASMgdYiNzaAVr9H3AZjZG0u8l3SBpt6SXJS1y9x3FDQ/IIneIjcyhVU5p4nsvk7TL3f8gSWb2c0k3SRo2lGbGrh84wd2tgW8jd2hKA7kjc2jKcJlr5iXoaZLeGtLeXakBrUTuEBuZQ0s0cwWcN6MHv/WZ2VJJS5s4DjAUuUNsZA4t0cwEvFvSuUPa50h6p7qTu6+RtEbiZRkUgtwhNjKHlmhmAn5Z0kwz+7iktyV9WdI/FTIqYHgdlbvx48cHtenTpwe1KVOmZNoHDx4M+rz33ntBra+vr+b3HTt2rOY4T3IdlTnE0/AE7O4DZrZM0gZJYyQ94u7bCxsZkIPcITYyh1Zp+G1IDR2Ml2UwRIOroEetzLnjCrh4MXJX5syheK1YBQ0AABrUzD1gAAU6/fTTg9qll14a1JYsWRLU5syZk2lv3Lgx6LN27dqgduDAgUz70KFDQZ+T7QoYaBdcAQMAkAATMAAACTABAwCQABMwAAAJsAgLSOS0007LtK+66qqgz1e/+tWgdv311we1rq6uTPuFF14I+uzcuTOo7dmzJ9NmwRUQD1fAAAAkwAQMAEACTMAAACTABAwAQAIswgIiOOOMM4LavHnzMu1vfetbQZ/58+cHtYGBgaC2Y8eOTPvo0aNBn7ydtmLuBQ8giytgAAASYAIGACABJmAAABJo6h6wmf1R0gFJRyUNuPvcIgYFjITcITYyh1YoYhHWte6+p3a3zjBmzJhM+yMf+UhDP2fZsmVBbdy4cUFt1qxZQe2OO+7ItO+7776gz6JFi4LaX//610x75cqVQZ8VK1aEg02jbXM3fvz4oHbFFVcEta997WuZ9tVXXx30yduZ6plnnglq69aty7TfeuutoM+7774b1FiENSptm7kUONc1j5egAQBIoNkJ2CU9Z2a/NrOleR3MbKmZdZtZd5PHAo4jd4iNzKFwzb4EfaW7v2NmZ0t63sz+1903De3g7mskrZEkM+P1LhSB3CE2MofCNTUBu/s7lY99ZvaMpMskbRr5u9rPeeedl2mfeuqpQZ+8e3h5mySceeaZmfYXv/jF5gZXw+7du4PaqlWrMu2FCxcGfQ4cOBDUfvvb32bav/zlL5scXWu0W+6qN7iYPXt20Gfx4sVBbcGCBZl23r3j119/Paht27YtqL300kuZ9v79+4M+3O9tXLtlrlGc6wa1y7mu4ZegzazLzD58/HNJfy/plaIGBuQhd4iNzKFVmrkCnizpGTM7/nP+w93/q5BRAcMjd4iNzKElGp6A3f0Pkv62wLEANZE7xEbm0Cq8DQkAgAQs5sKMdlgZOGfOnKD24osvZtqNvqG81fI2Zfj6178e1Pr7+2v+rN7e3qD25z//OdN+9dVXRzG60XN3a+kBKorMXeVlyIzzzz8/087bWODWW28NatX/n3lPK8r7v/zpT38a1J566qlMO2/xVldXV80x5J0PPvjgg6BWvZGBJB0+fHjEn90uYuSOc11zOuVcxxUwAAAJMAEDAJAAEzAAAAkwAQMAkEDHLcKaOHFiUNu8eXOmXb2opmjVx5Okffv2ZdrXXntt0OfIkSNBrV0XUdTjZFmE9bGPfSzTvuWWW4I+eU862r59e6Z96NChoM+8efOC2qRJk4Ja9c4+u3btCvpMnTo1qFU/dSZv16DqBSuS9Oabbwa17u7umn0GBgaCWmydsgiLc137YBEWAABthAkYAIAEmIABAEig4+4B57n55psz7c9//vNBn9/85jdBrfpJHHm2bt0a1PLuBx48eDDTvvjii4M+y5cvD2pLl+Y+mrQUyngPOM/YsWMz7SlTpgR98u6/7tmzJ9POuwd8+eWXB7VLL700qE2fPj3TPuuss4I+EyZMCGrVm3+MGTMm6JO3gUf1fTxJ+slPfpJpP/nkk0GfvPvJsXXKPeA8nOvS4B4wAABthAkYAIAEmIABAEig5gRsZo+YWZ+ZvTKkNtHMnjeznZWP4c0loAnkDrGROcRWcxGWmV0tqV/S4+4+u1L7V0l73X2lmd0laYK7f6/mwdp0YUK1M844I6jlbVCwevXqoLZkyZJM+ytf+UrQ54knnmhidCePkRbDlDl3H/pQ+Htt3uKmep5ElLcAKm9DgupFWHkbLOT9rOqNRc4555ygT/XCHUmaMWNGUHv22Wcz7RUrVgR9enp6glrspyYNl7syZ65RnOviaHgRlrtvkrS3qnyTpMcqnz8m6eZmBgdUI3eIjcwhtkbvAU92915Jqnw8u7ghAcMid4iNzKFlTmn1AcxsqaTyvoELpUTuEBuZw2g1egX8rplNlaTKx77hOrr7Gnef6+5zGzwWcBy5Q2xkDi3T6BXwOkm3S1pZ+bi2sBG1gffff7+ufvv376/Z5xvf+EZQy9shKPZClJIqRe7y/i8b/f/NWxDT398f1N57771Me9u2bUGfvIVg1TthXXnllUGf+fPnB7XzzjsvqMXcVS+iUmSuUZzr0qrnbUhPSHpJ0iwz221mSzQYxhvMbKekGyptoDDkDrGROcRW8wrY3RcN86XrCh4LcAK5Q2xkDrGxExYAAAkwAQMAkEDL34Z0MrvnnnuC2qc//elM+5prrgn6XH/99UHtueeeK2xcOLnlLXY6fPjwiG0p3PVKCndCuuSSS4I+eQuuqh8pJ0kvv/xypv2nP/0p6MMCnHLiXNcaXAEDAJAAEzAAAAkwAQMAkEDNpyEVerCSPCGkGRdccEGmvWXLlqDPvn37gtrGjRsz7e7u7qDPQw89FNTKvPnBSE9DKlIn5K4ep512WlD7whe+kGl/5zvfCfpceOGFQW3Dhg1B7Yc//GGmvX379qDP0aNHa46z1WLkrhMyx7mufg0/DQkAABSPCRgAgASYgAEASIAJGACABNiIo2CvvfZapr148eKgz6OPPhrUbrvtthHbktTV1RXUHn/88aDW29tba5joQBMnTgxqn/3sZzPtmTNnBn327t0b1DZt2hTUXn/99Uy7HRZcoXU41zWPK2AAABJgAgYAIAEmYAAAEqg5AZvZI2bWZ2avDKndY2Zvm9nWyp8FrR0mOg25Q2xkDrHV3AnLzK6W1C/pcXefXandI6nf3e8b1cE6YHeYesyePTuoPfDAA5n2ddfV9wzw1atXB7V7770303777bdHMbp4RtqRiNw1Z+zYsUHtxhtvDGp33313pv2JT3wi6PP0008HtVWrVgW1np6eTLtdn3w0XO7IXPE41w1qeCcsd98kKVwGCbQQuUNsZA6xNXMPeJmZbau8bDNhuE5mttTMus0s3PATGD1yh9jIHFqi0Qn4x5IukDRHUq+k+4fr6O5r3H2uu89t8FjAceQOsZE5tExdT0MysxmS1h+/L1Lv13L6cl9kGGeeeWamXf2UGin/Te1m4a2FF198MdO+4YYbmhtci9R6Kg25q191Ds4666ygz5133hnUqjdP6OvrC/pU37OT8u8L79+/v9Yw20KNtQczROZainPd/2voCtjMpg5pLpT0ynB9gaKQO8RG5tBKNbeiNLMnJP2dpI+a2W5J/yLp78xsjiSX9EdJ32zdENGJyB1iI3OIreYE7O6LcsoPt2AswAnkDrGROcTGTlgAACRQ1yKswg7GwoSmHD58OKidckr4IsbAwECm/bnPfS7o84tf/KKwcTWq1iKsonRC7saNG5dpX3PNNUGf73//+0Ht4osvzrTXr18f9Ln//nDh77Zt24Jade7aVYzcdULmWqlTznVcAQMAkAATMAAACTABAwCQABMwAAAJ1HwbEop3ySWXBLUvfelLmfZnPvOZoE/eIoQ8O3bsyLQ3bdo0itGh3eU96eiTn/xkpn3rrbcGfWbOnBnU3njjjUw7LyvVfaTyLLhCWpzrRsYVMAAACTABAwCQABMwAAAJMAEDAJAAi7AKNmvWrEx72bJlQZ9bbrklqE2ZMqWh4x09ejSo9fb2ZtrHjh1r6GejPY0fPz6oXXHFFZn2VVddFfQ5cOBAUFu7dm2mnbdrUFkeM4i4ONc1jytgAAASYAIGACCBmhOwmZ1rZhvNrMfMtpvZ8kp9opk9b2Y7Kx8ntH646BTkDrGROcRW82lIZjZV0lR332JmH5b0a0k3S1osaa+7rzSzuyRNcPfv1fhZpX1CSN59i0WLwseHVt8HmTFjRmFj6O7uDmr33ntvUFu3bl1hx2ylkZ5KQ+4GnX766UEt70lHy5cvz7SrN+aQpCeffDKoPfroo5n2zp07gz5HjhypOc4yGS53ZG4Q57riNfw0JHfvdfctlc8PSOqRNE3STZIeq3R7TINBBQpB7hAbmUNso7oHbGYzJH1K0mZJk929VxoMrqSzCx8dIHKH+MgcYqj7bUhmNl7SU5K+7e7vm9X3TGszWyppaWPDQ6cjd4iNzCGWuq6AzWysBgP5M3d/ulJ+t3LP5Pi9k76873X3Ne4+193nFjFgdA5yh9jIHGKqeQVsg7/+PSypx90fGPKldZJul7Sy8nFtzreXwuTJkzPtiy66KOjz4IMPBrULL7ywsDFs3rw50/7Rj34U9KneNEEq3xvP69UJuavHuHHjglpe7qo3KVixYkXQ59lnnw1qfX3ZuSRvs4NO0QmZ41zXXup5CfpKSbdJ+p2Zba3U7tZgGP/TzJZIelPSP7ZkhOhU5A6xkTlEVXMCdvf/kTTcTZDrih0OMIjcITYyh9jYCQsAgASYgAEASKDmTliFHizy7jATJ04MaqtXrw5qc+bMybTPP//8wsbwq1/9Kqjdf//9QW3Dhg2Z9qFDhwobQ7saaSesIpV5V6JTTgnvEk2aNCmodXV1ZdrVi6skqb+/v7iBlViM3HGuG8S5blDDO2EBAIDiMQEDAJAAEzAAAAmU9h7w5ZdfHtS++93vZtqXXXZZ0GfatGlFDUF/+ctfgtqqVasy7R/84AdBn4MHDxY2hjLjHjBSKNs9YM515cc9YAAA2ggTMAAACTABAwCQABMwAAAJ1P084HazcOHCumr12LFjR6a9fv36oM/AwEBQy3uT+b59+xoaAwDk4Vx38uIKGACABJiAAQBIoOYEbGbnmtlGM+sxs+1mtrxSv8fM3jazrZU/C1o/XHQKcofYyBxiq+ce8ICkO919i5l9WNKvzez5ytf+zd3va93w0MHIHWIjc4hq1DthmdlaSQ9KulJS/2hCyY5EGGo0OxKROxSl3tyRORSlkJ2wzGyGpE9J2lwpLTOzbWb2iJlNaG6IQD5yh9jIHGKoewI2s/GSnpL0bXd/X9KPJV0gaY6kXknhOvXB71tqZt1m1t38cNFpyB1iI3OIpa6XoM1srKT1kja4+wM5X58hab27z67xc3hZBifUeimQ3KEVRsodmUMrNPwStJmZpIcl9QwNpJlNHdJtoaRXmh0kcBy5Q2xkDrHVvAI2s/mS/lvS7yQdq5TvlrRIgy/JuKQ/Svqmu/fW+Fn8VogTalyJkDu0xHC5I3NolWEzV9bnAaP8eB4wUijb84BRfjwPGACANsIEDABAAkzAAAAkwAQMAEACTMAAACTABAwAQAJMwAAAJFDP4wiLtEfSG5I+Wvm8jBh7MaZHPBa5S6udxh4rd2QurXYa+7CZi7oRx4mDmnW7+9zoBy4AYy+vMv/9GXs5lfnvzthbj5egAQBIgAkYAIAEUk3AaxIdtwiMvbzK/Pdn7OVU5r87Y2+xJPeAAQDodLwEDQBAAtEnYDO70cxeNbNdZnZX7OOPhpk9YmZ9ZvbKkNpEM3vezHZWPk5IOcY8ZnaumW00sx4z225myyv1th97K5C5OMhdFrmLo8y5izoBm9kYSQ9J+gdJF0laZGYXxRzDKP27pBurandJesHdZ0p6odJuNwOS7nT3v5E0T9IdlX/nMoy9UGQuKnJXQe6iKm3uYl8BXyZpl7v/wd2PSPq5pJsij6Fu7r5J0t6q8k2SHqt8/pikm2OOqR7u3uvuWyqfH5DUI2maSjD2FiBzkZC7DHIXSZlzF3sCnibprSHt3ZVamUx2915p8D9e0tmJxzMiM5sh6VOSNqtkYy8ImUuA3JG7FMqWu9gTsOXUWIbdImY2XtJTkr7t7u+nHk8iZC4ycieJ3EVXxtzFnoB3Szp3SPscSe9EHkOz3jWzqZJU+diXeDy5zGysBsP4M3d/ulIuxdgLRuYiIncnkLuIypq72BPwy5JmmtnHzexUSV+WtC7yGJq1TtLtlc9vl7Q24VhymZlJelhSj7s/MORLbT/2FiBzkZC7DHIXSalz5+5R/0haIOn3kl6T9M+xjz/KsT4hqVfSBxr8jXaJpEkaXFG3s/JxYupx5ox7vgZf7tomaWvlz4IyjL1F/x5kLs7YyV3234PcxRl7aXPHTlgAACTATlgAACTABAwAQAJMwAAAJMAEDABAAkzAAAAkwAQMAEACTMAAACTABAwAQAL/BxRGv+lmaTSLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image0 = train_dataset.as_numpy_iterator().next()[0][:][:][0]\n",
    "train_dataset = prepare_data(train_dataset)\n",
    "_, ax = plt.subplots(1, 3, figsize = (8,4))\n",
    "ax[0].imshow(test_image0, cmap = 'gray')\n",
    "ax[1].imshow(train_dataset.as_numpy_iterator().next()[0][:][:][0], cmap = 'gray')\n",
    "ax[2].imshow(x_train[0], cmap = 'gray')\n",
    "print(train_dataset.as_numpy_iterator().next()[0].min(), train_dataset.as_numpy_iterator().next()[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2469c52-9437-440c-bf4c-50a3bba856a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the cnn model\n",
    "def make_cnn_model():\n",
    "    cnn_model = tf.keras.models.Sequential()\n",
    "    cnn_model.add(Conv2D(filters = 8, kernel_size = (3,3), input_shape = (img_height, img_width, 1)))\n",
    "    cnn_model.add(Conv2D(filters = 8, kernel_size = (3,3), activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling2D())\n",
    "    cnn_model.add(Dropout(0.25))\n",
    "    cnn_model.add(Conv2D(filters = 8, kernel_size = (3,3), activation = 'relu'))\n",
    "    cnn_model.add(Conv2D(filters = 8, kernel_size = (3,3), activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling2D())\n",
    "    cnn_model.add(Dropout(0.25))\n",
    "    cnn_model.add(Flatten())\n",
    "    cnn_model.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "    cnn_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                     loss = 'categorical_crossentropy',\n",
    "                     metrics = ['accuracy'])\n",
    "    #cnn_model.summary()\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c1b6296-d45f-40d6-a25c-2d09fb39c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the cnn model without softmax at the end\n",
    "def make_cnn_model_without_softmax(cnn_model):\n",
    "    cnn_no_softmax = tf.keras.models.Sequential()\n",
    "    cnn_no_softmax.add(Conv2D(filters = 8, kernel_size = (3,3), input_shape = (img_height, img_width, 1)))\n",
    "    cnn_no_softmax.add(Conv2D(filters = 8, kernel_size = (3,3), activation = 'relu'))\n",
    "    cnn_no_softmax.add(MaxPooling2D())\n",
    "    cnn_no_softmax.add(Dropout(0.25))\n",
    "    cnn_no_softmax.add(Conv2D(filters = 8, kernel_size = (3,3), activation = 'relu'))\n",
    "    cnn_no_softmax.add(Conv2D(filters = 8, kernel_size = (3,3), activation = 'relu'))\n",
    "    cnn_no_softmax.add(MaxPooling2D())\n",
    "    cnn_no_softmax.add(Dropout(0.25))\n",
    "    cnn_no_softmax.add(Flatten())\n",
    "    cnn_no_softmax.add(Dense(num_classes))\n",
    "    # init weights\n",
    "    cnn_no_softmax(tf.ones((1, img_height, img_width, 1)))\n",
    "    # copy weights\n",
    "    cnn_no_softmax.set_weights(cnn_model.get_weights())\n",
    "    return cnn_no_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "222fddad-de3c-4f0b-b1d4-7300a3190670",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-86685af2f003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_cnn_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcnn_no_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_cnn_model_without_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-da92f3ee978d>\u001b[0m in \u001b[0;36mmake_cnn_model_without_softmax\u001b[0;34m(cnn_model)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcnn_no_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# init weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcnn_no_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# copy weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcnn_no_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    378\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \"\"\"\n\u001b[0;32m--> 420\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    421\u001b[0m         inputs, training=training, mask=mask)\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m     name=None):\n\u001b[0;32m-> 1012\u001b[0;31m   return convolution_internal(\n\u001b[0m\u001b[1;32m   1013\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m       return op(\n\u001b[0m\u001b[1;32m   1143\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m           \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2594\u001b[0m     \u001b[0;31m# We avoid calling squeeze_batch_dims to reduce extra python function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m     \u001b[0;31m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2596\u001b[0;31m     return gen_nn_ops.conv2d(\n\u001b[0m\u001b[1;32m   2597\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2598\u001b[0m         \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    930\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "# load the model if it already exists\n",
    "# otherwise create and train it\n",
    "if(os.path.exists('mnist_cnn_model')):\n",
    "    cnn_model = tf.keras.models.load_model('mnist_cnn_model')\n",
    "else:\n",
    "    cnn_model = make_cnn_model()\n",
    "    cnn_history = cnn_model.fit(x_train,\n",
    "                                y_train,\n",
    "                                epochs = 20,\n",
    "                                validation_split = 0.2,\n",
    "                                verbose = 0,\n",
    "                                batch_size = 64)\n",
    "    cnn_model.save('mnist_cnn_model')\n",
    "    \n",
    "cnn_no_softmax = make_cnn_model_without_softmax(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25933735-a7ac-4b56-a2e9-7563f76c7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the cnn model\n",
    "test_loss, test_acc = cnn_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930cea4-ccd0-4e72-b443-ea55aa257bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare outputs of both cnn models\n",
    "real_img = np.expand_dims(x_train[1], 0)\n",
    "print(cnn_model(real_img))\n",
    "print(cnn_no_softmax(real_img))\n",
    "plt.imshow(real_img[0,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a4ab7-fadc-4071-b99f-a2783dc2fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make a generator model\n",
    "# from https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(7*7*256, use_bias = False, input_shape = (100, )))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)\n",
    "    model.add(Conv2DTranspose(128, (5, 5), strides = (1, 1), padding = 'same', use_bias = False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c35823-1402-457c-a574-c56f08f8568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make the discriminator model\n",
    "# from https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), strides = (2, 2), padding = 'same', input_shape = [28, 28, 1]))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb5d2e-ca30-416c-8492-155a6049423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions and optimizers\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2862ed-c43b-4507-8fe7-8f689eb73d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define augmentation for the generator\n",
    "def prepare_gen_data(ds):\n",
    "    ds = ds.map(lambda x: (data_augmentation(x, training = True)), num_parallel_calls = tf.data.AUTOTUNE)\n",
    "    return ds.prefetch(buffer_size = tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4e050-8a34-4932-b8a8-532460d9c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_image(model, epoch, test_input):\n",
    "    predictions = model(test_input, training = False)\n",
    "    fig = plt.figure(figsize = (4, 4))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i,:,:,0], cmap = 'gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e759d7f-4baa-41ab-ae4f-1f438f5d7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training loop\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = prepare_gen_data(dataset)\n",
    "\n",
    "# train step\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training = True)\n",
    "        real_output = discriminator(images, training = True)\n",
    "        fake_output = discriminator(generated_images, training = True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "def train(dataset, epochs, checkpoint_prefix, GAN_checkpoint):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "            \n",
    "        # save a checkpoint every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            GAN_checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            generate_and_save_image(generator, epoch, seed)\n",
    "        #print('Time for epoch {} is {} sec'.format(epoch+1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa3ec2-f9cf-4078-b94f-f81a985e4e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "generator = make_generator_model()\n",
    "checkpoint_dir = './GAN_training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "GAN_checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer,\n",
    "                                    discriminator_optimizer = discriminator_optimizer,\n",
    "                                    generator = generator,\n",
    "                                    discriminator = discriminator)\n",
    "\n",
    "if(os.path.exists('GAN_training_checkpoints')):\n",
    "    GAN_checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "else:\n",
    "    train(dataset, EPOCHS, checkpoint_prefix, GAN_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc26181e-9f71-47a1-bb9e-36164df20a22",
   "metadata": {},
   "source": [
    "generating an example image and comparing with a real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec09c7d-7492-4fe2-a9bf-ccd631d6b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1,100])\n",
    "generated_image = generator(noise, training = False)\n",
    "real_img = np.expand_dims(x_train[12], 0)\n",
    "# plot both images\n",
    "_, ax = plt.subplots(1, 2, figsize = (8,4))\n",
    "ax[0].imshow(generated_image[0,:,:,0], cmap = 'gray')\n",
    "ax[1].imshow(real_img[0,:,:,0], cmap = 'gray')\n",
    "# discriminator output\n",
    "print(discriminator(generated_image))\n",
    "print(discriminator(real_img))\n",
    "# cnn output\n",
    "print(cnn_model(generated_image))\n",
    "print(cnn_model(real_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044c819c-e59f-48dc-99f2-6b91bb46b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def target_function(x, index):\n",
    "    y = cnn_no_softmax(x)\n",
    "    y = tf.squeeze(y)\n",
    "    p = y[index]\n",
    "    print(\"p: \", p)\n",
    "    return p\n",
    "\n",
    "@tf.function\n",
    "def saliency(x, index):\n",
    "    #x_ = tf.cast(img, tf.float32) / 255.\n",
    "    #x = tf.Variable(x_)\n",
    "    with tf.GradientTape() as tape:\n",
    "        p = target_function(x, index)\n",
    "    grad = tape.gradient(p, x)\n",
    "    grad_abs = tf.math.abs(grad)\n",
    "    sal = (grad_abs - tf.math.reduce_min(grad_abs))/(tf.math.reduce_max(grad_abs) - tf.math.reduce_min(grad_abs))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d62ba-0a57-4783-9bbf-cb3974cfc03e",
   "metadata": {},
   "source": [
    "generating the saliency map for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86abc356-40d9-4b4e-bc0d-2155c957ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = tf.argmax(tf.squeeze(cnn_model(real_img))).numpy()\n",
    "x_ = tf.cast(real_img, tf.float32)\n",
    "x = tf.Variable(x_)\n",
    "sal = saliency(x, index)\n",
    "_, ax = plt.subplots(1, 2, figsize = (8,4))\n",
    "ax[0].imshow(real_img[0,:,:,0], cmap = 'gray')\n",
    "ax[1].imshow(sal[0,:,:,0], cmap = 'bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573eb5c1-1697-41a0-a3d7-ac19c9ea3703",
   "metadata": {},
   "source": [
    "# fooling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984a049-49be-4859-96ed-127d961f50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5ae74-12fe-4502-8389-b829af043f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a tensorflow version of the image: scaled to [0,1] and with the extra dimension for the channel\n",
    "x_ = tf.cast(real_img, tf.float32)\n",
    "\n",
    "# create a tensorflow variable with respect to which we can differentiatiate\n",
    "x = tf.Variable(x_) # initialize x with the original image\n",
    "\n",
    "# get the vector of predicted probabilities for the original image\n",
    "orig_y = cnn_model(x).numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25905b92-1a65-481c-955f-c83a4836885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def target_function2(x):\n",
    "    y = cnn_model(x)\n",
    "    y = tf.squeeze(y)\n",
    "    target_prob = y[target_class]\n",
    "    return target_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d25d8e-cdd1-4511-97e9-b8fdb340253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_function2(real_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b782dc0c-2753-43e6-b0d0-9df7a716393c",
   "metadata": {},
   "source": [
    "maximizing the target probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b418373-2d5b-4aa8-a03e-17350c6ee60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 600\n",
    "max_change = 0.005     # maximum change per iteration and pixel\n",
    "min_alpha = 100000     # a high value may be necessary to escape a local maximum\n",
    "close_enough = False\n",
    "epsilon = 1e-16\n",
    "iter = 0\n",
    "\n",
    "while not close_enough and iter < max_iter:\n",
    "    with tf.GradientTape() as tape:\n",
    "        p = target_function2(x)\n",
    "    grad = tape.gradient(p, x)\n",
    "    max_grad = tf.abs(tf.reduce_max(grad))\n",
    "    alpha = tf.minimum(100000, max_change/tf.maximum(max_grad, epsilon))\n",
    "    if(iter % 10 == 0):\n",
    "        #print(\"alpha: \", alpha, \" max_grad: \", max_grad)\n",
    "        pass\n",
    "    x.assign(x + alpha * grad)\n",
    "    x.assign(tf.clip_by_value(x, clip_value_min = 0, clip_value_max = 1))\n",
    "    iter += 1\n",
    "    if (p > 0.9999): # replace the 0 with the current value of the target_fkt\n",
    "        close_enough = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef1225-a657-49ac-8d9a-0d90d8bfc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prob = target_function2(x).numpy()\n",
    "print(\"probability of class \", target_class, \"is\", target_prob)\n",
    "print(orig_y)\n",
    "print(cnn_model(x).numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2afaf-b993-466e-91a6-7ebf432165df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(real_img[0,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d3ac6-d5f7-4d0b-a4cc-c85390455717",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[0,:,:,0], cmap = 'gray')\n",
    "discriminator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190ba71-2467-41a5-91b5-aa07910fec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    prob = target_function2(x)\n",
    "saliency = tape.gradient(prob, x)\n",
    "plt.imshow(saliency[0,:,:,0], cmap = 'bwr')\n",
    "print(tf.reduce_max(saliency))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13016472-adfc-4069-98e0-1b6d690243c2",
   "metadata": {},
   "source": [
    "$ g(x) = - \\ln P(x) + \\ln P(y|x) = - \\ln P(x) + \\ln f(x) $ \\\n",
    "use gradient descend to minimize $g(x)$ \\\n",
    "$ x_{\\text{new} } = x - \\alpha \\cdot \\nabla g(x) $ \\\n",
    "tf.GradientTape needs to evaluate the function to get the gradient. \\\n",
    "This is a problem because the discriminator can return negative values. \\\n",
    "To avoid evaluating the natural log at negative values the following property of its derivative can be used: \\\n",
    "$ \\frac{d}{dx} \\ln (-x) = \\frac{1}{-x} \\cdot -1 = \\frac{1}{x} = \\frac{d}{dx} \\ln (x) $ \\\n",
    "calculate the gradient of $ g $ explicitely: \\\n",
    "$ \\frac{d}{dx} g(x) = \\frac{d}{dx} \\ln (f(x)) - \\frac{d}{dx} \\ln(P(x)) = \\frac{1}{f(x)}f'(x) - \\frac{1}{P(x)}P'(x) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018bd6a-8617-441d-b217-725194bc5eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset x to original image\n",
    "# make a tensorflow version of the image: scaled to [0,1] and with the extra dimension for the channel\n",
    "x_ = tf.cast(real_img, tf.float32)\n",
    "\n",
    "# create a tensorflow variable with respect to which we can differentiatiate\n",
    "x = tf.Variable(x_) # initialize x with the original image\n",
    "\n",
    "# get the vector of predicted probabilities for the original image\n",
    "orig_y = cnn_model(x).numpy().squeeze()\n",
    "#plt.imshow(x[0,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30a475-704d-4445-9dc4-8ec8e6743992",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    p = discriminator(x)\n",
    "grad_p = tape.gradient(p, x)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    f = cnn_no_softmax(x)\n",
    "grad_f = tape.gradient(f, x)\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize = (8,4))\n",
    "ax[0].imshow(grad_p[0,:,:,0], cmap = 'bwr')\n",
    "ax[0].set_title(\"saliency map of the discriminator\")\n",
    "ax[1].imshow(grad_f[0,:,:,0], cmap = 'bwr')\n",
    "ax[1].set_title(\"saliency map of the classifier\")\n",
    "ax[2].imshow(x[0,:,:,0], cmap = 'gray')\n",
    "ax[2].set_title(\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe5a6d8-4b01-435e-a65b-674fc8a2919a",
   "metadata": {},
   "source": [
    "# algorithm version 1:\n",
    "## reducing the probability of the originaly prediction while increasing the discriminator output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8a88c-41f5-4a93-a857-c4a11070c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm\n",
    "max_iter = 600\n",
    "max_change = 0.005     # maximum change per iteration and pixel\n",
    "min_alpha = 100000     # a high value may be necessary to escape a local maximum\n",
    "close_enough = False\n",
    "epsilon = 1e-16\n",
    "iter = 0\n",
    "\n",
    "index = tf.argmax(tf.squeeze(cnn_model(x))).numpy()\n",
    "\n",
    "while not close_enough and iter < max_iter:\n",
    "    # derive discriminator\n",
    "    with tf.GradientTape() as tape:\n",
    "        p = discriminator(x)\n",
    "    grad_p = tape.gradient(p, x)\n",
    "    # derive classifier\n",
    "    with tf.GradientTape() as tape:\n",
    "        f = tf.squeeze(cnn_no_softmax(x))[index]\n",
    "    grad_f = tape.gradient(f, x)\n",
    "    \n",
    "    max_grad_p = tf.abs(tf.reduce_max(grad_p))\n",
    "    max_grad_f = tf.abs(tf.reduce_max(grad_f))\n",
    "    max_grad = tf.maximum(max_grad_p, max_grad_f)\n",
    "    alpha = tf.minimum(100000, max_change/tf.maximum(max_grad, epsilon))\n",
    "    \n",
    "    if(iter % 10 == 0):\n",
    "        #print(\"grad_p/p: \", grad_p/p, \" grad_f/f: \", grad_f/tf.reduce_max(f))\n",
    "        pass\n",
    "    \n",
    "    x.assign(x - alpha * ((grad_p/p) - (grad_f/tf.reduce_max(f))))\n",
    "    x.assign(tf.clip_by_value(x, clip_value_min = 0, clip_value_max = 1))\n",
    "    iter += 1\n",
    "    new_index = tf.argmax(tf.squeeze(cnn_model(x))).numpy()\n",
    "    if ((new_index != index) and (discriminator(x).numpy() > 0)): # done when the prediction has changed\n",
    "        close_enough = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869ea4b-80ea-4f59-aae1-2037f400662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize = (8,4))\n",
    "ax[0].imshow(x[0,:,:,0], cmap = 'gray')\n",
    "ax[1].imshow(real_img[0,:,:,0], cmap = 'gray')\n",
    "print(discriminator(x))\n",
    "print(cnn_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f2296-6db6-46b3-9bc0-4d12c4682fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    p = discriminator(x)\n",
    "grad_p = tape.gradient(p, x)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    f = cnn_no_softmax(x)\n",
    "grad_f = tape.gradient(f, x)\n",
    "\n",
    "_, ax = plt.subplots(1, 3, figsize = (8,4))\n",
    "ax[0].imshow(grad_p[0,:,:,0], cmap = 'bwr')\n",
    "ax[0].set_title(\"saliency map of the discriminator\")\n",
    "ax[1].imshow(grad_f[0,:,:,0], cmap = 'bwr')\n",
    "ax[1].set_title(\"saliency map of the classifier\")\n",
    "ax[2].imshow(x[0,:,:,0], cmap = 'gray')\n",
    "ax[2].set_title(\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85209d44-ade7-49fb-a75f-889a6566b6a1",
   "metadata": {},
   "source": [
    "# algorithm version 2\n",
    "increasing the output for a class that looks similar to the correct class\n",
    "for example:\n",
    "3 and 9,\n",
    "3 and 8,\n",
    "4 and 9,\n",
    "8 and 9,\n",
    "0 and 9\n",
    "while increasing the discriminator output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
